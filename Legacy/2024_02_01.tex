\documentclass{article}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage{MyNotations}
\title{WorkMeetings}
\date{}
\author{}

\hypersetup{
    colorlinks=true,
    citecolor=BleuLMS,
    linkcolor=BleuLMS,
    filecolor=BleuLMS,      
    urlcolor=GreenLMS,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

    \addbibresource{biblio.bib}

\begin{document}

\maketitle


\section{Progress report}

When moving from linear 1D elements to higher order or dimensions, the use of an element-wise architecture \parencite{liu_hidenn-fem_2023} appears more versatile. The shape functions are therefore built on each element as opposed to globally on the mesh. 

One way of combining those approaches would be to add an \emph{Assembly layer} before the interpolation layer that rebuilds the global shape functions at the structure's (mesh) scale as illustrated in \cref{fig:assembly}.

For each element, a sub-neural network would be built so that the output consists of the local shape functions $\Tilde{N}_i$ defined on the element. 

\underline{Important:} at this point, several versions of the same shape function (associated to the same node) can exist independently through different elements.

The \emph{Assembly layer} layer would then reconnect every local version $\Tilde{N}_i$ of a given global shape function $N_i$ associated to the $i-\text{th}$ node of the mesh. This layer would be a linear layer (\textbf{with bias of $-1$}) that inputs all the $\Tilde{N}_i$ and outputs a smaller layer of the $N_i$.
The weight matrix of such a layer would read
\begin{equation}
    w_{i,d\left(e-1\right)+k} = \begin{cases}
        1,\text{ if }k-\text{th node of element }e\text{ is node }i \\
        0,\text{ otherwise}
    \end{cases}
\end{equation}

\begin{figure}[bpt]
    \centering
    \includegraphics[width = 0.8\linewidth]{Schema/Diagram_NN_elementwise.drawio.pdf}
    \caption{Composition of shape functions.  The operation can be implemented using a linear layer with a weight matrix based on the connectivity table of given mesh and zero bias. This would allow using the existing interpolation layer consisting of nodal values.}
    \label{fig:assembly}
\end{figure}

\paragraph{Warning:}
When decomposition the shape functions and then assembling them, using an element-wise support for the functions leads to issues on the exact nodal coordinates for the assembled global shape functions as illustrated in \cref{fig:Discontinuous}.

\begin{figure}[hbpt]
    \centering
    \includegraphics[width=\linewidth]{Schema/Discontinuous_sum.pdf}
    \caption{Element-wise support, discontinuous sum}
    \label{fig:Discontinuous}
\end{figure}

Using ``leaking''shape functions that are defined outside of the element \parencite{zhang_hierarchical_2021} allow to bypass this issue as shown in \cref{fig:Continuous}. In that case the assembly layer will need to have the $-1$ biais shown in the last layer of the shape functions detailed by \cite{zhang_hierarchical_2021}.
\begin{itemize}
    \item[\faLightbulb] Check how to apply that idea in higher dimensions
\end{itemize}
\begin{figure}[hbpt]
    \centering
    \includegraphics[width=\linewidth]{Schema/Continuous_sum.pdf}
    \caption{Leaking shape functions, continuous sum}
    \label{fig:Continuous}
\end{figure}

\printbibliography[heading=bibintoc]
\end{document}
